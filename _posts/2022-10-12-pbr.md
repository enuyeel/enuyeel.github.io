---
layout: post
title: "Physically Based Rendering (WIP)"
permalink: /blog/:year/:month/:day/:title
---

The brightness of a surface is defined as its surface radiance. How do you then go about measuring the brightness of a surface? <br>
Imagine a sensor of some area at a distance to the surface of interest. If you move the sensor further away from the surface, you can intuitively tell it will receive less light. More formally, it's due to the solid angle $$\omega$$ with respect to each point on the surface decreasing. In other words, it's the area of the sensor staying the same, but the distance (radius of the sphere/hemisphere) between the sensor and the point on the surface increases. The solid angle $$\omega$$ is defined as the area $$A$$ over the radius squared $$r^2$$. <br>

$$\omega=\frac{A}{r^2}$$

By definition, it's inversely proportional to the radius squared $$r^2$$, and it's only the radius $$r$$ increasing in this scenario, hence the smaller solid angle $$\omega$$. <br>
If you increase the area of the surface, on the other hand, more light will enter the sensor, therefore increasing the perceived brightness.<br>
Therefore, when you define the brightness of a surface, you need to normalize with respect to both the area of the surface as well as the solid angle subtended by each point in the surface with respect to your sensor. <br>
This brings us to the definition that surface radiance $$L$$ is defined as flux $$\Phi$$ per unit (foreshortened) area $$dAcos\theta_{r}$$ per unit solid angle $$d\omega$$. It’s good to be reminded now that the surface radiance $$L$$ depends on the reflectance properties of a surface, which I’ll go more in-depth about later in the article. <br>

$$L=\frac{d^2\Phi}{(dAcos\theta_{r})d\omega}$$

Don't let the complex Mathematical notation throw you off, though! If we consider the solid angle $$\omega$$ and the area $$A$$ to be infinitesimal, we can effectively translate the surface to a single point (or fragment) and the solid angle into a directional vector (from a fragment to one of the points on the hemisphere's curved surface). Although the actual implementations won't closely resemble the above formula, this radiometry concept is the backbone of Physically Based Rendering (PBR), and many different models describing the way that light scatters at surfaces are somewhat directly and indirectly derived from it. <br>
<br>\*\*\*<br><br>
It is to be noted a single point in the macro level can be modeled as a collection of small microfacets in the micro level, hence the term Microfacet Model. If I rephrase it a little bit, our seemingly flat fragment on the surface at the macro-level is then really a collective of individual microfacets at the micro-level, and it's their aggregate (integral, in the mathematical expression) behavior against the light that determines the observed scattering. <br>
It is our interest how not just a single, but every microfacet on a differential area $$dA$$ (or a fragment) scatters any incoming lights in all different directions (technically only the directions covered by the upper hemisphere), thus a need for integrating the surface response over a hemisphere. We can however assume that a single incoming light will hit off every microfacet within the hemisphere at the same incident angle. <br>
However, it is computationally heavy to compute integration for every shaded fragment (multiple integrations if the scene consists of more than one light source), so it's best if our surface response which consists of diffuse and specular components effectively boils down to a statistical approximation. <br>
For a refresher, the moment a light ray hits a surface, it gets split into both a reflection part which doesn't enter the surface and gets directly reflected and a refraction part which enters the surface and either gets absorbed or later emerges from the surface at a distance. The former is responsible for the specular component, while the latter is responsible for the diffuse component. It should be mentioned that conductors (e.g purely metallic materials) have no diffuse component. <br>
There are many different versions of general microfacet model equations available online. It should be noted that since my PBR implementation for the most part is based on Google Filament, I'll borrow their notations from now on unless otherwise specified. <br>
<br>\*\*\*<br><br>
This is an excerpt from Shree Nayar's lecture on Bidirectional Reflectance Distribution Function (BRDF). "There are two directions that are important to us. One is the direction from which the light arrives, and the second is the direction in which light is being reflected and observed. So, to represent the reflectance properties of any material, we want to be able to describe its properties both in terms of the illumination direction and the viewing direction of the reflection direction. So, that brings us to the BRDF." <br>
BRDF is a 4-dimensional function that takes in 2 sets of zenith and azimuth angles, where each set represents the incoming light and the outgoing viewing direction. <br>

$$f(\theta_i,\phi_i,\theta_r,\phi_r)$$

Often its parameterization is simplified with the said two vectors accordingly. It then describes the reflectance properties of a surface, or in layman's terms, surface response. <br>
The complete surface response BRDF can then be expressed as a combination of two individual BRDF terms as such:<br>

$$f(v,l)=f_d(v,l)+f_r(v,l)$$

Each of $$f_d$$ and $$f_r$$ represents the diffuse component and the specular component correspondingly, and we'll first look into the specular BRDF term $$f_r$$. <br>

$$f_r(v,l)=\frac{D(h,\alpha)G(v,l,\alpha)F(v,h,f0)}{4(n\cdot v)(n\cdot l)}$$

There's a lot to digest here, but this form of BRDF is known as the Torrance-Sparrow model. A little bit of everything is involved in the derivation of this formula, notably, the definition of surface radiance and BRDF. Each of the three different terms on the numerator has its specific role for this BRDF, and before I'll go over them individually, it should be mentioned that the derivation itself depends on neither the particular $$D$$ (normal distribution function) nor a particular $$F$$ (Fresnel function), which means we can pretty much swap those terms in and out with whatever valid formulations we have. <br>
According to the PBR book, the normal distribution function (NDF, in short) $$D(h,\alpha)$$ gives the differential area of microfacets aligned with the given half vector $$h$$, where the variation of microfacet normals is described by the roughness parameter $$\alpha$$. In its normalized form, NDF is a probability density function over a solid angle (half vector, if loosely spoken), and it serves as a weighting function to scale the brightness of specular reflection in BRDF. Since it returns probability density, not a probability, in a scenario where the surface is smooth (low roughness value) and the half vector is close to the (macro) surface normal, its value is expected to shoot up much higher than 1. <br>
*[TODO : How different NDFs require a different definition for a roughness parameter.]* <br>
We'll stick with Trowbridge-Reitz for our choice of NDF, as it has higher tails around grazing angle compared to the other models, say Beckmann-Spizzichino, and it closely matches the properties of real-world surfaces. <br>

$$D_{Trowbridge-Reitz(Isotropic)}(h,\alpha)=\frac{\alpha^2}{\pi((n\cdot h)^2(\alpha^2-1)+1)^2}$$

You can check out the different shapes of NDFs [**here**](https://www.desmos.com/calculator/ewcjfzvfnk), and play around with them to figure out which would be the fittest for your application. <br>
*[TODO : Integral of not-normalized NDFs, and try yielding 1 with the additional term(s).]* <br>
*[TODO : Add isotropic Trowbridge-Reitz to Desmos, and possibly a few more variations of NDFs.]* <br>

<br>

*[TODO : Tidy up references.]* <br>

- [Radiometric Concepts by Shree K. Nayar](https://youtu.be/tflz0loWhIY)
- [BRDF by Shree K. Nayar](https://youtu.be/R9iZzaXUaK4)
- [Google Filament Documentation](https://google.github.io/filament/Filament.html)
- [Google Filament shading_model_standard.fs](https://google.github.io/filament/Filament.html)
- [Google Filament brdf.fs](https://github.com/google/filament/blob/main/shaders/src/brdf.fs)
- [PBR Book 8.4 Microfacet Models](https://www.pbr-book.org/3ed-2018/Reflection_Models/Microfacet_Models)
- [PBR Book 5.6.1 The BRDF](https://www.pbr-book.org/3ed-2018/Color_and_Radiometry/Surface_Reflection#TheBRDF)
- [learnopengl.com PBR Part I "Theory"](https://learnopengl.com/PBR/Theory)
- [How Is The NDF Really Defined?](https://www.reedbeta.com/blog/hows-the-ndf-really-defined/)
- [Specular BRDF Reference](http://graphicrants.blogspot.com/2013/08/specular-brdf-reference.html)
- [Sampling microfacet BRDF](https://agraphicsguy.wordpress.com/2015/11/01/sampling-microfacet-brdf/)
- [Notes On Importance Sampling](https://blog.tobias-franke.eu/2014/03/30/notes_on_importance_sampling.html)
- [Correct Specular Term of the Cook-Torrance / Torrance-Sparrow Model](https://computergraphics.stackexchange.com/questions/3946/correct-specular-term-of-the-cook-torrance-torrance-sparrow-model)
- [Microfacet Distribution Function: To Change or Not to Change, That Is the Question](https://www.scitepress.org/Papers/2021/102527/102527.pdf)
- [Physically Based Shading at Disney](https://media.disneyanimation.com/uploads/production/publication_asset/48/asset/s2012_pbs_disney_brdf_notes_v3.pdf)