---
layout: post
title: "IBL (WIP)"
permalink: /blog/:year/:month/:day/:title
---

This article is still in a draft state!

<br>

***

<br>

Difference between the approximation derived from the Monte Carlo integration and the expected value of $$f(x)$$ is what defines the *variance*. We can simply increase the number of samples to minimize the variance, which eventually unfolds to visible artifacts in rendering. However, if we're simply unable to afford the computational resources necessary for obtaining more samples, we should come up with a compromise method to produce results that still somewhat converge to the expected value. Variance reduction techniques, especially the importance sampling technique that I'll be discussing in this article require knowledge of the function being integrated.<br>

<!-- <p style="text-align: center;"></p><br> -->

$$X_i=a+\xi(b-a)$$

$$\frac{b-a}{N}\sum_{i=1}^{N}f(X_i)$$

Above is the basic Montel Carlo estimator, where $$\xi$$ is a uniformly distributed random number in the range $$[0, 1]$$. If we were to integrate a constant function (e.g. $$y=1$$) with the formula above, each approximation would yield the same result every time we run the integration, which means the variance is $$0$$. We can take a hint from this observation, and try to come up with a way to "turn a non-constant function into a constant function" as phrased by [Scratchapixel16d].<br> 
[MAYAUX19] and [Scratchapixel16d] go over and expand on this idea with the general Monte Carlo estimator and explain how providing a matching $$pdf(x)$$ for $$f(x)$$, in other words, a $$pdf(x)$$ which is proportional (ideally, exactly proportional) to $$f(x)$$, would effectively reduce the integrand to a constant, achieving a goal of $$0$$ variance:<br>

$$\frac{1}{N}\sum_{i=1}^{N}\frac{f(X_i)}{pdf(X_i)}$$

$$\frac{f(X_i)}{pdf(X_i)}=\frac{f(X_i)}{c\cdot f(X_i)}=\frac{1}{c}$$

From [Scratchapixel16b], "(...) the invert of the CDF for the normal distribution varies very little for small values of X and value of X close to 1."<br>
From [Scratchapixel16d], "(...) let's explain where the term importance sampling comes from. (...) If the PDF is nonuniform, the density of samples may vary across the domain of integration (creating more samples in areas where the PDF is high, and less where it is low)."<br>
Isotropic Trowbridge-Reitz (GGX):<br>

$$D(h)=\frac{\alpha^2}{\pi((n\cdot h)^2(\alpha^2-1)+1)^2}$$

$$\int_{\Omega}^{}D(h,\alpha)(n\cdot h)d\omega_{h}=1$$

$$pdf(d\omega_{h})=D(h,\alpha)(n\cdot h)$$

$$D(\theta,\alpha)=\frac{\alpha^2}{\pi(cos^2\theta(\alpha^2-1)+1)^2}$$

$$\int_{0}^{2\pi}\int_{0}^{\frac{\pi}{2}}D(\theta,\alpha)cos\theta sin\theta d\theta d\phi=1$$

$$pdf(\theta,\phi)=D(\theta,\alpha)cos\theta sin\theta$$

Deriving marginal probability density function $$pdf(\theta)$$:<br>

$$pdf(\theta)=\int_{0}^{2\pi}pdf(\theta,\phi)d\phi=2\pi\cdot pdf(\theta,\phi)=\frac{2\alpha^2cos\theta sin\theta}{(cos^2\theta(\alpha^2-1)+1)^2}$$

Deriving cumulative distribution function $$cdf(\theta)$$:<br>

$$cdf(\theta)=\int_{0}^{\theta}\frac{2\alpha^2cos(t)sin(t)}{(cos^2(t)(\alpha^2-1)+1)^2}dt$$

TODO: derivation steps skipped<br>

$$=\frac{\alpha^2}{cos^2\theta(\alpha^2-1)^2+(\alpha^2-1)}-\frac{1}{\alpha^2-1}=\xi_\theta$$

From [Scratchapixel16a], "Remember that a probability can only take on a value within the range [0,1]. The idea behind the inverse sampling method is to somehow invert the CDF so that the output of the CDF (the probability) becomes the input of the inverted CDF, and the input of the CDF (...) becomes the output of the inverted CDF"<br>

$$\theta=cos^{-1}\sqrt{\frac{1-\xi_\theta}{\xi_\theta(\alpha^2-1)+1}}$$

From [Frank14], "We can retrieve the PDF for $$\phi$$ with the conditional probability $$p(\phi\vert\theta)$$, the conditional probability density function:"<br>
From [MAYAUX19], "The probability $$p(\phi\vert\theta)$$ of having a sample along the direction $$\phi$$ knowing we already have a sample along the angle $$\theta$$."<br>

$$pdf(\phi\vert\theta)=\frac{pdf(\theta,\phi)}{pdf(\theta)}=\frac{pdf(\theta,\phi)}{2\pi\cdot pdf(\theta,\phi)}=\frac{1}{2\pi}$$

$$cdf(\phi\vert\theta)=\int_{0}^{\phi}\frac{1}{2\pi}d\phi=\frac{\phi}{2\pi}=\xi_\phi$$

$$\phi=2\pi\cdot\xi_\phi$$

From [Scratchapixel16c], "Remember that in terms of visual artifact, deterministic techniques such as the Rieman sum produce aliasing while Monte Carlo integration produces noise. (...) we will first talk about a method that is somewhere in between random and regular distributed samples. This method is called stratified sampling (...) Quasi-Monte Carlo integration relies on sequences of points with particular properties. (...) the goal is to generate sequences of samples that are not exactly uniformly distributed (uniformly distributed samples cause aliasing) and yet appear to have some regularity in the way they are spaced. (...) Random sequences have a high discrepancy while a sequence of uniformly distributed samples would have a zero discrepancy. The sort of sequences we are after is the ones with the lowest possible discrepancy but not a zero discrepancy."<br>

<br>

***

<br>

**Bibliograph**

<br>

[[**Karis13**]](https://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf) Brian Karis. 2013. Real Shading in Unreal Engine 4. https://blog.selfshadow.com/publications/s2013-shading-course/karis/s2013_pbs_epic_notes_v2.pdf<br>
[[**Vries18**]](https://learnopengl.com/PBR/IBL/Specular-IBL) Joey de Vries. 2018. Specular IBL. https://learnopengl.com/PBR/IBL/Specular-IBL<br>
[[**Frank14**]](https://blog.tobias-franke.eu/2014/03/30/notes_on_importance_sampling.html) Tobias Alexander Franke. 2014. Notes On Importance Sampling. https://blog.tobias-franke.eu/2014/03/30/notes_on_importance_sampling.html<br>
[[**Taboga21**]](https://www.statlect.com/glossary/marginal-probability-density-function) Marco Taboga. 2021. Marginal Probability Density Function. https://www.statlect.com/glossary/marginal-probability-density-function<br>
[[**Wolfe17**]](https://blog.demofox.org/2017/08/05/generating-random-numbers-from-a-specific-distribution-by-inverting-the-cdf/) Alan Wolfe. 2017. Generating Random Numbers From a Specific Distribution By Inverting the CDF. https://blog.demofox.org/2017/08/05/generating-random-numbers-from-a-specific-distribution-by-inverting-the-cdf/<br>
[[**MAYAUX19**]](https://patapom.com/blog/Math/ImportanceSampling/) Beno√Æt MAYAUX. 2019. Importance Sampling. https://patapom.com/blog/Math/ImportanceSampling/<br>
[[**Cao15**]](https://agraphicsguynotes.com/posts/sample_microfacet_brdf/) Jiayin Cao. 2015. Sampling Microfacet BRDF. https://agraphicsguynotes.com/posts/sample_microfacet_brdf/
https://computergraphics.stackexchange.com/questions/12161/pdf-of-brdf-respecting-the-spherical-coordinates<br>
[[**Scratchapixel16a**]](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-simulation.html) Scratchapixel. 2016. Monte Carlo Simulation. https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/monte-carlo-simulation.html<br>
[[**Scratchapixel16b**]](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/inverse-transform-sampling-method.html) Scratchapixel. 2016. The Inverse Transform Sampling Method. https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-mathematical-foundations/inverse-transform-sampling-method.html<br>
[[**Scratchapixel16c**]](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/introduction-quasi-monte-carlo.html) Scratchapixel. 2016. Monte Carlo Methods In Practice. https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/introduction-quasi-monte-carlo.html<br>
[[**Scratchapixel16d**]](https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/variance-reduction-methods.html) Scratchapixel. 2016. Variance Reduction Methods: A Quick Introduction to Importance Sampling. https://www.scratchapixel.com/lessons/mathematics-physics-for-computer-graphics/monte-carlo-methods-in-practice/variance-reduction-methods.html<br>

<br>

